# LLM-Hallucination-Papers
Collection of LLM hallucination Papers

## Contents
- [Conference Papers](#conference-papers)
	- 2024:  [ACL](#acl-2024),  [EMNLP](#emnlp-2024),  [NAACL](#naacl-2024)
	- 2023:  [ACL](#acl-2023),  [EMNLP](#emnlp-2023)
	- 2022:  [ACL](#acl-2022),  [EMNLP](#emnlp-2022),  [NAACL](#naacl-2022)
	- 2021:  [ACL](#acl-2021),  [EMNLP](#emnlp-2021),  [NAACL](#naacl-2021)

## Conference Papers

###  ACL 2024
Coming ...
### EMNLP 2024
Coming ...
### NAACL 2024
- Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision [[pdf]](https://aclanthology.org/2024.naacl-long.23/)
- On Large Language Modelsâ€™ Hallucination with Regard to Known Facts [[pdf]](https://aclanthology.org/2024.naacl-long.60/)
- Language Models Hallucinate, but May Excel at Fact Verification [[pdf]](https://aclanthology.org/2024.naacl-long.62/)
- Can Knowledge Graphs Reduce Hallucinations in  LLMs? : A Survey [[pdf]](https://aclanthology.org/2024.naacl-long.219/)
- TofuEval: Evaluating Hallucinations of  LLMs on Topic-Focused Dialogue Summarization [[pdf]](https://aclanthology.org/2024.naacl-long.251/)
- Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination? [[pdf]](https://aclanthology.org/2024.naacl-long.424/)
- Hallucination Diversity-Aware Active Learning for Text Summarization [[pdf]](https://aclanthology.org/2024.naacl-long.479/)

### ACL 2023

### EMNLP 2023

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE0NTMyODY0NjcsMTg2Njc2MTA5MywtMT
Q4Mjk4MzkzMywyODUzMjU4MzAsLTg5MzkwOTIxMiwzNDMxODIx
MTYsLTEwNDA0NjM3MDgsLTEwNDA0NjM3MDgsNjUxNDA2NTksMT
IwMzczMTEyMiwyMDM2NDA4MTAsNzI5Njc0ODQwLC04ODcyMTUy
NDAsMjA5NDI4NzAxNiwtOTUzNTc2NTAyLC02NjY0MDM3MzMsOT
I3Nzk3MTE4LDIxMjAwNDIxNTAsLTUxMTc3ODY0OSwyMDQ5OTIx
NDkzXX0=
-->