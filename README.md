# LLM-Hallucination-Papers

## Contents

|  Year  | [Conference](#conference-papers) |                      |                      |
| :---:  |    :----:        |        :---:         |        :---:         |
|  2024  | [ACL](#acl-2024) | [EMNLP](#emnlp-2024) | [NAACL](#naacl-2024) |
|  2023  | [ACL](#acl-2023) | [EMNLP](#emnlp-2023) |                      |
|  2022  | [ACL](#acl-2022) | [EMNLP](#emnlp-2022) | [NAACL](#naacl-2022) |
|  2021  | [ACL](#acl-2021) | [EMNLP](#emnlp-2021) | [NAACL](#naacl-2021) |

## Conference Papers

###  ACL 2024
Coming ...
### EMNLP 2024
Coming ...
### NAACL 2024
- Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision [[pdf]](https://aclanthology.org/2024.naacl-long.23/)
- On Large Language Modelsâ€™ Hallucination with Regard to Known Facts [[pdf]](https://aclanthology.org/2024.naacl-long.60/)
- Language Models Hallucinate, but May Excel at Fact Verification [[pdf]](https://aclanthology.org/2024.naacl-long.62/)
- Can Knowledge Graphs Reduce Hallucinations in  LLMs? : A Survey [[pdf]](https://aclanthology.org/2024.naacl-long.219/)
- TofuEval: Evaluating Hallucinations of  LLMs on Topic-Focused Dialogue Summarization [[pdf]](https://aclanthology.org/2024.naacl-long.251/)
- Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination? [[pdf]](https://aclanthology.org/2024.naacl-long.424/)
- Hallucination Diversity-Aware Active Learning for Text Summarization [[pdf]](https://aclanthology.org/2024.naacl-long.479/)

### ACL 2023

### EMNLP 2023

<!--stackedit_data:
eyJoaXN0b3J5IjpbNjA4ODk5MDE3LC00MzU3NzIxMTcsNTcxMz
A4NDk3LDE4NjY3NjEwOTMsLTE0ODI5ODM5MzMsMjg1MzI1ODMw
LC04OTM5MDkyMTIsMzQzMTgyMTE2LC0xMDQwNDYzNzA4LC0xMD
QwNDYzNzA4LDY1MTQwNjU5LDEyMDM3MzExMjIsMjAzNjQwODEw
LDcyOTY3NDg0MCwtODg3MjE1MjQwLDIwOTQyODcwMTYsLTk1Mz
U3NjUwMiwtNjY2NDAzNzMzLDkyNzc5NzExOCwyMTIwMDQyMTUw
XX0=
-->